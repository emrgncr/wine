--- server/fd.c
+++ server/fd.c
@@ -409,6 +412,26 @@ static const int user_shared_data_timeout = 16;
 static void set_user_shared_data_time(void)
 {
     timeout_t tick_count = monotonic_time / 10000;
+    unsigned __int64 tsc, qpc_bias, qpc_freq = user_shared_data->QpcFrequency;
+    unsigned int aux, qpc_shift = user_shared_data->QpcShift;
+    unsigned int qpc_bypass = user_shared_data->QpcBypassEnabled;
+
+    if (!(qpc_bypass & SHARED_GLOBAL_FLAGS_QPC_BYPASS_ENABLED))
+        tsc = 0;
+#if defined(__i386__) || defined(__x86_64__)
+    else if (qpc_bypass & SHARED_GLOBAL_FLAGS_QPC_BYPASS_USE_RDTSCP)
+        tsc = __rdtscp(&aux);
+    else
+    {
+        if (qpc_bypass & SHARED_GLOBAL_FLAGS_QPC_BYPASS_USE_MFENCE)
+            __asm__ __volatile__ ( "mfence" : : : "memory" );
+        if (qpc_bypass & SHARED_GLOBAL_FLAGS_QPC_BYPASS_USE_LFENCE)
+            __asm__ __volatile__ ( "lfence" : : : "memory" );
+        tsc = __rdtsc();
+    }
+#endif
+
+    qpc_bias = ((monotonic_time * qpc_freq / 10000000) << qpc_shift) - tsc;
 
     /* on X86 there should be total store order guarantees, so volatile is enough
      * to ensure the stores aren't reordered by the compiler, and then they will
@@ -427,6 +450,7 @@ static void set_user_shared_data_time(void)
     user_shared_data->TickCount.LowPart   = tick_count;
     user_shared_data->TickCount.High1Time = tick_count >> 32;
     *(volatile ULONG *)&user_shared_data->TickCountLowDeprecated = tick_count;
+    user_shared_data->QpcBias = qpc_bias;
 #else
     __atomic_store_n(&user_shared_data->SystemTime.High2Time, current_time >> 32, __ATOMIC_SEQ_CST);
     __atomic_store_n(&user_shared_data->SystemTime.LowPart, current_time, __ATOMIC_SEQ_CST);
@@ -440,6 +464,7 @@ static void set_user_shared_data_time(void)
     __atomic_store_n(&user_shared_data->TickCount.LowPart, tick_count, __ATOMIC_SEQ_CST);
     __atomic_store_n(&user_shared_data->TickCount.High1Time, tick_count >> 32, __ATOMIC_SEQ_CST);
     __atomic_store_n(&user_shared_data->TickCountLowDeprecated, tick_count, __ATOMIC_SEQ_CST);
+    __atomic_store_n(&user_shared_data->QpcBias, qpc_bias, __ATOMIC_SEQ_CST);
 #endif
 }
 
--- server/fd.c
+++ server/fd.c
@@ -403,9 +403,18 @@ static struct list rel_timeout_list = LIST_INIT(rel_timeout_list); /* sorted rel
 timeout_t current_time;
 timeout_t monotonic_time;
 
+struct hypervisor_shared_data *hypervisor_shared_data = NULL;
 struct _KUSER_SHARED_DATA *user_shared_data = NULL;
 static const int user_shared_data_timeout = 16;
 
+/* 128-bit multiply a by b and return the high 64 bits, same as __umulh */
+static UINT64 multiply_tsc(UINT64 a, UINT64 b)
+{
+    UINT64 ah = a >> 32, al = (UINT32)a, bh = b >> 32, bl = (UINT32)b, m;
+    m = (ah * bl) + (bh * al) + ((al * bl) >> 32);
+    return (ah * bh) + (m >> 32);
+}
+
 static void set_user_shared_data_time(void)
 {
     timeout_t tick_count = monotonic_time / 10000;
@@ -428,7 +437,13 @@ static void set_user_shared_data_time(void)
     }
 #endif
 
-    qpc_bias = ((monotonic_time * qpc_freq / 10000000) << qpc_shift) - tsc;
+    if (!(qpc_bypass & SHARED_GLOBAL_FLAGS_QPC_BYPASS_USE_HV_PAGE))
+        qpc_bias = ((monotonic_time * qpc_freq / 10000000) << qpc_shift) - tsc;
+    else
+    {
+        tsc = multiply_tsc(tsc, hypervisor_shared_data->QpcMultiplier);
+        qpc_bias = monotonic_time - tsc;
+    }
 
     /* on X86 there should be total store order guarantees, so volatile is enough
      * to ensure the stores aren't reordered by the compiler, and then they will
@@ -447,7 +462,10 @@ static void set_user_shared_data_time(void)
     user_shared_data->TickCount.LowPart   = tick_count;
     user_shared_data->TickCount.High1Time = tick_count >> 32;
     *(volatile ULONG *)&user_shared_data->TickCountLowDeprecated = tick_count;
-    user_shared_data->QpcBias = qpc_bias;
+    if (qpc_bypass & SHARED_GLOBAL_FLAGS_QPC_BYPASS_USE_HV_PAGE)
+        hypervisor_shared_data->QpcBias = qpc_bias;
+    else
+        user_shared_data->QpcBias = qpc_bias;
 #else
     __atomic_store_n(&user_shared_data->SystemTime.High2Time, current_time >> 32, __ATOMIC_SEQ_CST);
     __atomic_store_n(&user_shared_data->SystemTime.LowPart, current_time, __ATOMIC_SEQ_CST);
@@ -461,7 +479,10 @@ static void set_user_shared_data_time(void)
     __atomic_store_n(&user_shared_data->TickCount.LowPart, tick_count, __ATOMIC_SEQ_CST);
     __atomic_store_n(&user_shared_data->TickCount.High1Time, tick_count >> 32, __ATOMIC_SEQ_CST);
     __atomic_store_n(&user_shared_data->TickCountLowDeprecated, tick_count, __ATOMIC_SEQ_CST);
-    __atomic_store_n(&user_shared_data->QpcBias, qpc_bias, __ATOMIC_SEQ_CST);
+    if (qpc_bypass & SHARED_GLOBAL_FLAGS_QPC_BYPASS_USE_HV_PAGE)
+        __atomic_store_n(&hypervisor_shared_data->QpcBias, qpc_bias, __ATOMIC_SEQ_CST);
+    else
+        __atomic_store_n(&user_shared_data->QpcBias, qpc_bias, __ATOMIC_SEQ_CST);
 #endif
 }
 
